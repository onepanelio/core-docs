<!doctype html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-alpha.62">
<link rel="preconnect" href="https://www.google-analytics.com">
<script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-106005416-5","auto"),ga("send","pageview")</script>
<script async src="https://www.google-analytics.com/analytics.js"></script>
<link rel="search" type="application/opensearchdescription+xml" title="The open source, end-to-end vision AI platform" href="/opensearch.xml">
<script src="/js/gtm.js"></script><title data-react-helmet="true">Training with built-in models | The open source, end-to-end vision AI platform</title><meta data-react-helmet="true" name="docsearch:version" content="current"><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:title" content="Training with built-in models | The open source, end-to-end vision AI platform"><meta data-react-helmet="true" name="description" content="Onepanel - Training with built-in models"><meta data-react-helmet="true" property="og:description" content="Onepanel - Training with built-in models"><meta data-react-helmet="true" property="og:url" content="https://docs.onepanel.ai/docs/getting-started/use-cases/computervision/annotation/cvat/cvat_annotation_model"><link data-react-helmet="true" rel="shortcut icon" href="/img/favicon.png"><link data-react-helmet="true" rel="preconnect" href="https://BH4D9OD16A-dsn.algolia.net" crossorigin="true"><link data-react-helmet="true" rel="canonical" href="https://docs.onepanel.ai/docs/getting-started/use-cases/computervision/annotation/cvat/cvat_annotation_model"><link rel="stylesheet" href="/styles.ed37b51c.css">
<link rel="preload" href="/styles.18972f97.js" as="script">
<link rel="preload" href="/runtime~main.e1b70c34.js" as="script">
<link rel="preload" href="/main.f6281ef0.js" as="script">
<link rel="preload" href="/1.8e58a5cf.js" as="script">
<link rel="preload" href="/2.24894898.js" as="script">
<link rel="preload" href="/62.74c495ad.js" as="script">
<link rel="preload" href="/65.7a25f0f3.js" as="script">
<link rel="preload" href="/935f2afb.9e2d08e3.js" as="script">
<link rel="preload" href="/17896441.b2db10f9.js" as="script">
<link rel="preload" href="/aeca0258.691f2d4f.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<nav class="navbar navbar--light navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><div aria-label="Navigation bar toggle" class="navbar__toggle" role="button" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" width="30" height="30" viewBox="0 0 30 30" role="img" focusable="false"><title>Menu</title><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></div><a class="navbar__brand" href="/"><img class="navbar__logo" src="/img/icon.png" alt="Onepanel logo"><strong class="navbar__title"></strong></a><a class="navbar__item navbar__link" href="/docs/getting-started/quickstart">Getting Started</a><a class="navbar__item navbar__link" href="/docs/reference/overview">User Guide</a><a class="navbar__item navbar__link" href="/docs/deployment/overview">Operator Manual</a><a class="navbar__item navbar__link" href="/docs/api-sdk/overview">APIs and SDKs</a><a href="https://github.com/onepanelio/core/releases" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Releases</a><a href="https://github.com/onepanelio/core/milestones?direction=asc&amp;sort=due_date&amp;state=open" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Roadmap</a></div><div class="navbar__items navbar__items--right"><a target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-community-label">Join the community:</a><a href="https://join.slack.com/t/onepanel-ce/shared_invite/zt-eyjnwec0-nLaHhjif9Y~gA05KuX6AUg" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-icon-link header-slack-link"></a><a href="https://github.com/onepanelio/core" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-icon-link header-github-link"></a><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span><span class="DocSearch-Button-Key">⌘</span><span class="DocSearch-Button-Key">K</span></button></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div><div class="navbar-sidebar"><div class="navbar-sidebar__brand"><a class="navbar__brand" href="/"><img class="navbar__logo" src="/img/icon.png" alt="Onepanel logo"><strong class="navbar__title"></strong></a></div><div class="navbar-sidebar__items"><div class="menu"><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" href="/docs/getting-started/quickstart">Getting Started</a></li><li class="menu__list-item"><a class="menu__link" href="/docs/reference/overview">User Guide</a></li><li class="menu__list-item"><a class="menu__link" href="/docs/deployment/overview">Operator Manual</a></li><li class="menu__list-item"><a class="menu__link" href="/docs/api-sdk/overview">APIs and SDKs</a></li><li class="menu__list-item"><a href="https://github.com/onepanelio/core/releases" target="_blank" rel="noopener noreferrer" class="menu__link">Releases</a></li><li class="menu__list-item"><a href="https://github.com/onepanelio/core/milestones?direction=asc&amp;sort=due_date&amp;state=open" target="_blank" rel="noopener noreferrer" class="menu__link">Roadmap</a></li><li class="menu__list-item"><a target="_blank" rel="noopener noreferrer" class="menu__link header-community-label">Join the community:</a></li><li class="menu__list-item"><a href="https://join.slack.com/t/onepanel-ce/shared_invite/zt-eyjnwec0-nLaHhjif9Y~gA05KuX6AUg" target="_blank" rel="noopener noreferrer" class="menu__link header-icon-link header-slack-link"></a></li><li class="menu__list-item"><a href="https://github.com/onepanelio/core" target="_blank" rel="noopener noreferrer" class="menu__link header-icon-link header-github-link"></a></li></ul></div></div></div></nav><div class="main-wrapper"><div class="docPage_2gpo"><div class="docSidebarContainer_3_JD" role="complementary"><div class="sidebar_2urC"><div class="menu menu--responsive menu_5FrY"><button aria-label="Open Menu" aria-haspopup="true" class="button button--secondary button--sm menu__button" type="button"><svg aria-label="Menu" class="sidebarMenuIcon_Dm3K" xmlns="http://www.w3.org/2000/svg" height="24" width="24" viewBox="0 0 32 32" role="img" focusable="false"><title>Menu</title><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" href="/docs/reference/overview">User guide</a></li><li class="menu__list-item"><a class="menu__link menu__link--sublist" href="#!">Workspaces</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/reference/workspaces/launching">Launching a Workspace</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/reference/workspaces/pause-and-resume">Pause and resume</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/reference/workspaces/upgrade">Switching node pool</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/reference/workspaces/delete">Terminating a Workspace</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/reference/workspaces/templates">Workspace Templates</a></li></ul></li><li class="menu__list-item"><a class="menu__link menu__link--sublist" href="#!">Workflows</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/reference/workflows/execute">Executing a Workflow</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/reference/workflows/templates">Workflow Templates</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/reference/workflows/artifacts">Working with artifacts</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/reference/workflows/create">Creating a Workflow Template</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/reference/workflows/metrics">Persisting training metrics</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/reference/workflows/tensorboard">Accessing TensorBoard</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/reference/workflows/hyperparameter-tuning">Hyperparameter tuning</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/reference/workflows/troubleshooting">Troubleshooting Workflows</a></li></ul></li><li class="menu__list-item"><a class="menu__link menu__link--sublist menu__link--active" href="#!">CVAT Workspace</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/getting-started/use-cases/computervision/annotation/cvat/cvat_quick_guide">Getting started with image and video annotation</a></li><li class="menu__list-item"><a aria-current="page" class="menu__link menu__link--active active" tabindex="0" href="/docs/getting-started/use-cases/computervision/annotation/cvat/cvat_annotation_model">Training with built-in models</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/getting-started/use-cases/computervision/annotation/cvat/cvat_automatic_annotation">Using trained models for automatic annotation</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/getting-started/use-cases/computervision/annotation/cvat/adding_custom_model">Adding custom training Workflows</a></li></ul></li><li class="menu__list-item"><a class="menu__link menu__link--sublist" href="#!">JupyterLab Workspace</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/reference/jupyterlab/overview">JupyterLab overview</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/reference/jupyterlab/git">Using Git and browsing GitHub</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/reference/jupyterlab/tensorboard">Using TensorBoard</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/reference/jupyterlab/debugging">Using the built-in debugger</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/reference/jupyterlab/language-server">Auto completion and language features</a></li></ul></li></ul></div></div></div><main class="docMainContainer_3EyW"><div class="container padding-vert--lg docItemWrapper_1EkI"><div class="row"><div class="col docItemCol_2ASc"><div class="docItemContainer_3QWW"><article><div><span class="badge badge--secondary">Version: Next</span></div><header><h1 class="docTitle_1Lrw">Training with built-in models</h1></header><div class="markdown"><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="training-with-built-in-models-from-cvat"></a>Training with built-in models from CVAT<a aria-hidden="true" tabindex="-1" class="hash-link" href="#training-with-built-in-models-from-cvat" title="Direct link to heading">#</a></h2><ol><li><p>Click on <strong>Open</strong> for a task you want to train a model on.</p><p><img alt="Open task" src="/assets/images/cvat_open-f720c7c0bf600fa4cefeb1de339d730d.png"></p></li><li><p>Click on <strong>Job #X</strong>, where X could be any job number. Annotate few frames. For testing you can just annotate one frame. But ideally you want to have thousands of objects to train a deep learning model on. Alternatively, you can just run pre-annotation if your labels are common ones.</p></li><li><p>Click on <strong>Actions</strong> for a task you want to train a model on. Then, click on <strong>Execute training Workflow</strong>.</p><p><img alt="Select training workflow" src="/assets/images/cvat_select_workflow_execution-ec9d8925cf9110729a481ea6088aa571.png"></p></li><li><p>Select a training Workflow Template. By default, you can use <strong>TF Object Detection Training</strong> for object detection or <strong>MaskRCNN Training</strong> for semantic segmentation.</p><p><img alt="Train a model from CVAT" src="/assets/images/tf-object-detection-b6a97ad0987f3362c789cfb36af1ebce.png"></p><div class="admonition admonition-tip alert alert--success"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="12" height="16" viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</h5></div><div class="admonition-content"><p>Note you can easily add your own models as well. See our <a href="/docs/getting-started/use-cases/computervision/annotation/cvat/adding_custom_model">documentation</a> for more information on adding custom models. </p></div></div></li><li><p>Update hyperparameters and settings depending on your model and data. See below for more information.</p></li><li><p>You can optionally select the checkpoint path from previously trained model or leave this field empty.</p></li><li><p>Click <strong>Submit</strong>. This will execute the Onepanel Workflow for selected model. You can see Workflow logs by going to Workflow execution page. You can find the URL for the same in the notification card.</p><p><img alt="Workflow URL" src="/assets/images/execution_url-d1c171f8a2d8e9af476838254702195d.png"></p><p>Trained model and other outputs will be stored on cloud storage and will be synced with CVAT locally so that you can use this to pre-annotate other frames. </p><div class="admonition admonition-note alert alert--secondary"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="16" viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</h5></div><div class="admonition-content"><p>You can also use this trained model to run pre-annotation in CVAT. See our <a href="/docs/getting-started/use-cases/computervision/annotation/cvat/cvat_automatic_annotation">documentation</a> for more information on pre-annotation.</p></div></div></li></ol><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="tensorflow-object-detection-models"></a>TensorFlow Object Detection models<a aria-hidden="true" tabindex="-1" class="hash-link" href="#tensorflow-object-detection-models" title="Direct link to heading">#</a></h2><p>You can use any of the models that we support for TensorFlow Object Detection API to train your custom pre-annotation models. Here, we provide a brief explanation on how to choose one model over another based on your needs. Some models are faster than others, whereas some are more accurate than others.  We hope this information will help you choose the right model for your task. </p><p><img alt="TensorFlow Object Detection Workflow" src="/assets/images/tf-object-detection-b6a97ad0987f3362c789cfb36af1ebce.png"></p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="tfod-hyperparameters"></a>TFOD hyperparameters<a aria-hidden="true" tabindex="-1" class="hash-link" href="#tfod-hyperparameters" title="Direct link to heading">#</a></h3><p>You can specify some arguments in the <code>Hyperparameters</code> field seperated by new line. </p><p>Here is a sample for Tensorflow Object Detection API: </p><div class="mdxCodeBlock_1XEh"><div class="codeBlockContent_1u-d"><button type="button" aria-label="Copy code to clipboard" class="copyButton_10dd">Copy</button><div tabindex="0" class="prism-code language-bash codeBlock_3iAC"><div class="codeBlockLines_b7E3" style="color:#bfc7d5;background-color:#292d3e"><div class="token-line" style="color:#bfc7d5"><span class="token plain">num-steps</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token number" style="color:rgb(247, 140, 108)">100</span></div></div></div></div></div><p>Details:</p><ul><li>num-steps : number of steps to train your model for. If left empty, Onepanel will pick the recommended defaults for that model.</li><li>batch-size : batch size for the training</li><li>initial-learning-rate : initial learning rate for the model. We recommend you do not change this.</li><li>num-clones (default=1): number of GPUs to train the model </li><li>schedule-step-1: step 1 for linear learning rate decay</li><li>schedule-step-2: step 2 for linear learning rate decay</li></ul><p>Note that you need to set <code>num-clones</code> to <code>4</code> (number of GPUs) if you select a node pool with say 4 GPUs (Tesla V100).</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="choosing-the-right-model"></a>Choosing the right model<a aria-hidden="true" tabindex="-1" class="hash-link" href="#choosing-the-right-model" title="Direct link to heading">#</a></h3><ul><li><p>We currently support several faster-rcnn models. All of these models are similar except that of the backbone used for the feature extraction. The backbones used are, in increasing order of complexity (i.e more layers), ResNet50, ResNet101, InceptionResNetV2. As the model complexity increases the computation requirement will also increase. If you have very complicated data (i.e hundreds of annotations in one image), then it is recommended that you choose complex model (i.e InceptionResNetV2).</p></li><li><p>Faster-rcnn models are generally more accurate than ssd models. However, sometimes you are better off using ssd models if your data is easy to learn (i.e 1 or 2 bounding box per image).</p></li></ul><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="frcnn-nas-coco"></a>frcnn-nas-coco:<a aria-hidden="true" tabindex="-1" class="hash-link" href="#frcnn-nas-coco" title="Direct link to heading">#</a></h4><ul><li>If you are using <code>frcnn-nas-coco</code>, then choose a machine with at least 2 GPUs as this model requires more memory. A machine with 1 GPU will throw an error.</li></ul><p>This is a type of faster-rcnn model with NAS backbone. If you are not sure about which model to use then we recommend you use SSD based model (i.e ssd-mobilenet-v2).</p><p>For how to set epochs, you can take a look at first model since both models are faster-rcnn based.</p><p>Note that current implementation of faster-rcnn in TensorFlow Object Detection API does not support batch training. That is, you shouldn&#x27;t change <code>batch_size</code>.</p><p><strong><em>Defaults</em></strong>: batch_size: 1, learning_rate: 0.0003, epochs=10000</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="frcnn-res101-coco"></a>frcnn-res101-coco:<a aria-hidden="true" tabindex="-1" class="hash-link" href="#frcnn-res101-coco" title="Direct link to heading">#</a></h4><p>This is a type of faster-rcnn model with ResNet101 backbone. If you are not sure about which model to use then we recommend you use SSD based model (i.e ssd-mobilenet-v2). </p><p>For how to set epochs, you can take a look at first model since both models are faster-rcnn based.</p><p>Note that current implementation of faster-rcnn in TensorFlow Object Detection API does not support batch training. That is, you shouldn&#x27;t change <code>batch_size</code>.</p><p><strong><em>Defaults</em></strong>: batch_size: 1, learning_rate: 0.0003, epochs=10000</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="frcnn-res101-lowp"></a>frcnn-res101-lowp<a aria-hidden="true" tabindex="-1" class="hash-link" href="#frcnn-res101-lowp" title="Direct link to heading">#</a></h4><p>This is a type of faster-rcnn model with ResNet101 backbone with low number of proposals. If you are not sure about which model to use then we recommend you use SSD based model (i.e ssd-mobilenet-v2). If you are looking for more complex and accurate model then check out frcnn-res101-coco or frcnn-inc-resv2-atr-coco.</p><p>For how to set epochs, you can take a look at first model since both models are faster-rcnn based.</p><p>Note that current implementation of faster-rcnn in TensorFlow Object Detection API does not support batch training. That is, you shouldn&#x27;t change <code>batch_size</code>.</p><p><strong><em>Defaults</em></strong>: batch_size: 1, learning_rate: 0.0003, epochs=10000</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="frcnn-res50-coco"></a>frcnn-res50-coco<a aria-hidden="true" tabindex="-1" class="hash-link" href="#frcnn-res50-coco" title="Direct link to heading">#</a></h4><p>This is a type of faster-rcnn model with ResNet50 backbone. If you are not sure about which model to use then we recommend you use SSD based model (i.e ssd-mobilenet-v2). If you are looking for more complex and accurate model then check out frcnn-res101-coco or frcnn-inc-resv2-atr-coco.</p><p>For how to set epochs, you can take a look at first model since both models are faster-rcnn based.</p><p>Note that current implementation of faster-rcnn in TensorFlow Object Detection API does not support batch training. That is, you shouldn&#x27;t change <code>batch_size</code>.</p><p><strong><em>Defaults</em></strong>: batch_size: 1, learning_rate: 0.0003, epochs=10000</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="ssd-mobilenet-v2-coco"></a>ssd-mobilenet-v2-coco<a aria-hidden="true" tabindex="-1" class="hash-link" href="#ssd-mobilenet-v2-coco" title="Direct link to heading">#</a></h4><p>SSD-based networks such as <code>ssd-mobilenet-v2</code> are faster than faster-rcnn based models. However, they are not as accurate as faster-rcnn based models. This model is generally recommended since its accurate and fast enough. If you don&#x27;t know much about your data or the complexity of your data, then we recommend you go with this model.</p><p>You will find the pre-trained model and config file for ssd-mobilenetv2 model trained on COCO dataset.</p><p>This model is a good place to start if you don&#x27;t have any specific model in mind. If you are data is very complicated (i.e many annotations per image) then you should prefer faster-rcnn models over ssd.</p><p>Depending upon your data, you can set epochs to train your model. There is no standard value which can work for all datasets. You generally have to try different number of epochs to get the best model. Ideally, you do so by monitoring loss of your model while training. But if you are looking for a recommendation. Then, we recommend you set epochs as follows: (number of images / batch_size (default: 24)) * 1000. For instance, if you have 100 images, then your epochs will be 4000 (rounded). Note that the model will be trained using a pre-trained model, so you don&#x27;t need to train as long as you would have to when not using the pre-trained model.</p><p><strong><em>Defaults</em></strong>: batch_size: 24, learning_rate: 0.004, epochs=10000</p><p>Note that same instructions apply for <strong>ssd-mobilenet-v1</strong> and <strong>ssd-mobilenet-lite</strong>. The only difference is the backbone model (i.e mobilenet v1) that they use.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="maskrcnn-model"></a>MaskRCNN model<a aria-hidden="true" tabindex="-1" class="hash-link" href="#maskrcnn-model" title="Direct link to heading">#</a></h2><p>MaskRCNN is a popular model for segmentation tasks. We use <a href="https://github.com/matterport/Mask_RCNN" target="_blank" rel="noopener noreferrer">this</a> implementation of MaskRCNN for training and inference.</p><p>The process to train a Mask-RCNN model on CVAT is similar to the above process except that you need to select Mask-RCNN after clicking on Create Annotation Model.</p><p><img alt="MaskRCNN Workflow" src="/assets/images/maskrcnn-training-aa9ea4c5cc2866ddd9135e01a85c3f80.png"></p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="maskrcnn-hyperparameters"></a>MaskRCNN hyperparameters<a aria-hidden="true" tabindex="-1" class="hash-link" href="#maskrcnn-hyperparameters" title="Direct link to heading">#</a></h3><p>Even though you don&#x27;t need to enter any other parameters to start the training of Mask-RCNN, it is recommended that you pass correct epochs according your data. Mask-RCNN is a very deep model which takes too much time to train and also to get enough accuracy.
We allow you to set epochs for three different parts of the model. These parts are called <code>stage1</code>, <code>stage2</code> and <code>stage3</code>. You can set corresponding epochs as follows:</p><div class="mdxCodeBlock_1XEh"><div class="codeBlockContent_1u-d"><button type="button" aria-label="Copy code to clipboard" class="copyButton_10dd">Copy</button><div tabindex="0" class="prism-code language-bash codeBlock_3iAC"><div class="codeBlockLines_b7E3" style="color:#bfc7d5;background-color:#292d3e"><div class="token-line" style="color:#bfc7d5"><span class="token plain">stage-1-epochs</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token number" style="color:rgb(247, 140, 108)">1</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">stage-2-epochs</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token number" style="color:rgb(247, 140, 108)">2</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">stage-3-epochs</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token number" style="color:rgb(247, 140, 108)">3</span></div></div></div></div></div><p>If you have few images (few hundreds), then we recommend you set total epochs (stage1+stage2+stage3) less than 10. We advise you set more epochs for stage1 than others. As your data size increases or the complexity of your data increases you might want to increase epochs. </p><p>If you have ~1000 images then you don&#x27;t have to set any parameters, CVAT will take care of it.</p></div></article><div class="margin-vert--xl"><div class="row"><div class="col"><a href="https://github.com/onepanelio/core-docs/tree/master/docs/getting-started/use-cases/computervision/annotation/cvat/cvat_annotation_model.md" target="_blank" rel="noreferrer noopener"><svg fill="currentColor" height="1.2em" width="1.2em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 40 40" style="margin-right:0.3em;vertical-align:sub"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div></div></div><div class="margin-vert--lg"><nav class="pagination-nav" aria-label="Blog list page navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/docs/getting-started/use-cases/computervision/annotation/cvat/cvat_quick_guide"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">« Getting started with image and video annotation</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/docs/getting-started/use-cases/computervision/annotation/cvat/cvat_automatic_annotation"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Using trained models for automatic annotation »</div></a></div></nav></div></div></div><div class="col col--3"><div class="tableOfContents_3SO_"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#training-with-built-in-models-from-cvat" class="table-of-contents__link">Training with built-in models from CVAT</a></li><li><a href="#tensorflow-object-detection-models" class="table-of-contents__link">TensorFlow Object Detection models</a><ul><li><a href="#tfod-hyperparameters" class="table-of-contents__link">TFOD hyperparameters</a></li><li><a href="#choosing-the-right-model" class="table-of-contents__link">Choosing the right model</a></li></ul></li><li><a href="#maskrcnn-model" class="table-of-contents__link">MaskRCNN model</a><ul><li><a href="#maskrcnn-hyperparameters" class="table-of-contents__link">MaskRCNN hyperparameters</a></li></ul></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container"><div class="text--center"><div>Copyright © 2021 Onepanel, Inc.</div></div></div></footer></div>
<script src="/styles.18972f97.js"></script>
<script src="/runtime~main.e1b70c34.js"></script>
<script src="/main.f6281ef0.js"></script>
<script src="/1.8e58a5cf.js"></script>
<script src="/2.24894898.js"></script>
<script src="/62.74c495ad.js"></script>
<script src="/65.7a25f0f3.js"></script>
<script src="/935f2afb.9e2d08e3.js"></script>
<script src="/17896441.b2db10f9.js"></script>
<script src="/aeca0258.691f2d4f.js"></script>
</body>
</html>